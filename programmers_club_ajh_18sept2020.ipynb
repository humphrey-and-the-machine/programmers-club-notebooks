{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3 notebook for Programmer's Club 18 Sept 2020\n",
    "\n",
    "# import the usual data science and general things\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time, random, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import what we need from Scikit-Learn\n",
    "from sklearn import neighbors, neural_network\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "\n",
    "# import several cutting edge tree-based learners\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of astronomical objects: 20000\n",
      "columns: ['psfMag_r' 'fiberMag_r' 'fiber2Mag_r' 'petroMag_r' 'deVMag_r' 'expMag_r'\n",
      " 'modelMag_r' 'cModelMag_r' 'class']\n",
      "unique classes: ['GALAXY' 'STAR']\n"
     ]
    }
   ],
   "source": [
    "#from astroquery.sdss import SDSS  # enables direct queries to the SDSS database\n",
    "#this example was taken from https://github.com/LSSTC-DSFP/LSSTC-DSFP-Sessions\n",
    "# also presented by Tunde\n",
    "\n",
    "#STAR_GALquery = \"\"\"SELECT TOP 20000\n",
    "#                p.psfMag_r, p.fiberMag_r, p.fiber2Mag_r, p.petroMag_r, \n",
    "#                p.deVMag_r, p.expMag_r, p.modelMag_r, p.cModelMag_r, \n",
    "#                s.class\n",
    "#                FROM PhotoObjAll AS p JOIN specObjAll s ON s.bestobjid = p.objid\n",
    "#                WHERE p.mode = 1 AND s.sciencePrimary = 1 AND p.clean = 1 AND s.class != 'QSO'\n",
    "#                ORDER BY p.objid ASC\"\"\"\n",
    "\n",
    "#stars_gals = SDSS.query_sql(STAR_GALquery)\n",
    "#stars_gals[\"class\"]=np.array(stars_gals[\"class\"], dtype=np.str)\n",
    "\n",
    "# the above code throws errors on my Python 2.7 and 3.6.8 installations, which I will have to fix later ...\n",
    "# -- thanks to Tom Scott for running it and sending the results\n",
    "df = pd.read_csv('stars_gals')\n",
    "\n",
    "# summary of the data:\n",
    "print('number of astronomical objects:',len(df))\n",
    "print('columns:',df.columns.values)\n",
    "print('unique classes:',df['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>psfMag_r</th>\n",
       "      <th>fiberMag_r</th>\n",
       "      <th>fiber2Mag_r</th>\n",
       "      <th>petroMag_r</th>\n",
       "      <th>deVMag_r</th>\n",
       "      <th>expMag_r</th>\n",
       "      <th>modelMag_r</th>\n",
       "      <th>cModelMag_r</th>\n",
       "      <th>class</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.460980</td>\n",
       "      <td>-0.456968</td>\n",
       "      <td>-0.387287</td>\n",
       "      <td>-0.655817</td>\n",
       "      <td>-0.719171</td>\n",
       "      <td>-0.669434</td>\n",
       "      <td>-0.601619</td>\n",
       "      <td>-0.655157</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.169385</td>\n",
       "      <td>-0.068947</td>\n",
       "      <td>-0.116434</td>\n",
       "      <td>0.200199</td>\n",
       "      <td>0.249467</td>\n",
       "      <td>0.147178</td>\n",
       "      <td>0.219833</td>\n",
       "      <td>0.219415</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.306565</td>\n",
       "      <td>0.193642</td>\n",
       "      <td>0.259301</td>\n",
       "      <td>0.367189</td>\n",
       "      <td>0.334939</td>\n",
       "      <td>0.329573</td>\n",
       "      <td>0.403310</td>\n",
       "      <td>0.340907</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.507000</td>\n",
       "      <td>1.512229</td>\n",
       "      <td>1.527126</td>\n",
       "      <td>1.591933</td>\n",
       "      <td>1.376655</td>\n",
       "      <td>1.435021</td>\n",
       "      <td>1.515309</td>\n",
       "      <td>1.514541</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.246063</td>\n",
       "      <td>1.230142</td>\n",
       "      <td>1.262818</td>\n",
       "      <td>0.992363</td>\n",
       "      <td>0.755588</td>\n",
       "      <td>0.917236</td>\n",
       "      <td>0.994422</td>\n",
       "      <td>0.883807</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   psfMag_r  fiberMag_r  fiber2Mag_r  petroMag_r  deVMag_r  expMag_r  \\\n",
       "0 -0.460980   -0.456968    -0.387287   -0.655817 -0.719171 -0.669434   \n",
       "1 -0.169385   -0.068947    -0.116434    0.200199  0.249467  0.147178   \n",
       "2  0.306565    0.193642     0.259301    0.367189  0.334939  0.329573   \n",
       "3  1.507000    1.512229     1.527126    1.591933  1.376655  1.435021   \n",
       "4  1.246063    1.230142     1.262818    0.992363  0.755588  0.917236   \n",
       "\n",
       "   modelMag_r  cModelMag_r   class  Target  \n",
       "0   -0.601619    -0.655157  GALAXY       1  \n",
       "1    0.219833     0.219415    STAR       0  \n",
       "2    0.403310     0.340907  GALAXY       1  \n",
       "3    1.515309     1.514541  GALAXY       1  \n",
       "4    0.994422     0.883807  GALAXY       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing before we apply machine learning\n",
    "\n",
    "# create binary target column from class name strings\n",
    "df['Target'] = (df['class'] == 'GALAXY').astype(int)\n",
    "# check it worked\n",
    "df[['Target','class']]\n",
    "\n",
    "# make list of columns to use as training features\n",
    "cols = df.columns.values.tolist()\n",
    "cols.remove('class')\n",
    "cols.remove('Target')\n",
    "\n",
    "# standardize / scale the features to be used for model training\n",
    "# note: magnitudes and colours often don't need scaling since they are already logarithmic\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[cols])\n",
    "df[cols] = scaler.transform(df[cols])\n",
    "# let's looks at the results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "average accuracy over 10 runs: 0.9685  time elaped: 18.09 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest from Scikit-Learn: classify objects according to star (0) or galaxy (1)\n",
    "# start with the usual train / test split\n",
    "acc = 0\n",
    "n_iter = 10\n",
    "#state_list = [random.randrange(1, 9999, 1) for _ in range(10)] \n",
    "state_list = [3019, 2454, 4539, 2734, 9868, 2706, 2267, 9736, 4497, 659]\n",
    "\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(0,n_iter)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.25, random_state = state_list[i])\n",
    "\n",
    "    # define a model\n",
    "    clf = RandomForestClassifier(n_estimators=50,n_jobs=3)\n",
    "    \n",
    "    # fit the model to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict object classes for the test data \n",
    "    preds_bin = clf.predict(X_test)\n",
    "    # evaluate accuracy:    \n",
    "    acc += accuracy_score(y_test,preds_bin) / n_iter\n",
    "\n",
    "t1 = time.time()\n",
    "acc_rf = acc\n",
    "t_rf = t1-t0\n",
    "# average accuracy over n_iter runs\n",
    "print(type(clf).__name__)\n",
    "print('average accuracy over '+str(n_iter)+' runs:',np.round(acc,4),' time elaped:',np.round(t1-t0,2),'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining useful function\n",
    "def eval_results():\n",
    "    if (acc - acc_rf) >= 0: plus = '+'\n",
    "    else: plus = ''\n",
    "    acc_diff_str = '('+plus+str(np.round(acc - acc_rf,4))+').'\n",
    "    t_diff_str = '('+str(int((t1-t0)/t_rf*100.))+'%)'\n",
    "    print(type(clf).__name__)\n",
    "    print('average accuracy over '+str(n_iter)+' runs:',np.round(acc,4),acc_diff_str+' time elaped:',np.round(t1-t0,2),'s',t_diff_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoostClassifier\n",
      "average accuracy over 10 runs: 0.9691 (+0.0006). time elaped: 7.25 s (40%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# now lets try Catboost (for readability I refrain from defining classes or functions when repeating code blocks)\n",
    "\n",
    "acc = 0\n",
    "n_iter = 10\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(0,n_iter)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.25,random_state = state_list[i])\n",
    "    clf = CatBoostClassifier(logging_level='Silent',thread_count=3, n_estimators=50)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)\n",
    "    acc += accuracy_score(y_test,preds_bin) / n_iter\n",
    "\n",
    "t1 = time.time()\n",
    "eval_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier\n",
      "average accuracy over 10 runs: 0.9702 (+0.0018). time elaped: 4.57 s (25%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# now lets try Light GBM\n",
    "\n",
    "acc = 0\n",
    "n_iter = 10\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(0,n_iter)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.25,random_state = state_list[i])\n",
    "    clf = lgb.LGBMClassifier(n_jobs = 3,n_estimators=50)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)\n",
    "    acc += accuracy_score(y_test,preds_bin) / n_iter\n",
    "\n",
    "t1 = time.time()\n",
    "eval_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier\n",
      "average accuracy over 10 runs: 0.9636 (-0.0048). time elaped: 11.63 s (64%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost (not ideal for this problem, but included for completeness)\n",
    "\n",
    "acc = 0\n",
    "n_iter = 10\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(0,n_iter)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.25,random_state = state_list[i])\n",
    "    clf = xgb.XGBClassifier(n_estimators=50,n_jobs=3,tree_method='exact')\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)  \n",
    "    acc += accuracy_score(y_test,preds_bin) / n_iter\n",
    "\n",
    "t1 = time.time()\n",
    "eval_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "average accuracy over 10 runs: 0.9682 (-0.0003). time elaped: 8.69 s (48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Scikit-Learn ExtraTreesClassifier\n",
    "\n",
    "acc = 0\n",
    "n_iter = 10\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(0,n_iter)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.25,random_state = state_list[i])\n",
    "    clf = ExtraTreesClassifier(n_estimators=50,n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)  \n",
    "    acc += accuracy_score(y_test,preds_bin) / n_iter\n",
    "\n",
    "t1 = time.time()\n",
    "eval_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:48<00:00,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier\n",
      "average accuracy over 10 runs: 0.9634 (-0.005). time elaped: 48.42 s (267%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingClassifier\n",
    "# (apparently doesn't have an n_jobs parameter --> slow)\n",
    "\n",
    "acc = 0\n",
    "n_iter = 10\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(0,n_iter)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.25,random_state = state_list[i])\n",
    "    clf = GradientBoostingClassifier(n_estimators=50)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)  \n",
    "    acc += accuracy_score(y_test,preds_bin) / n_iter\n",
    "\n",
    "t1 = time.time()\n",
    "eval_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:15<00:00, 37.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "average accuracy over 10 runs: 0.9704 (+0.0019). time elaped: 375.3 s (2074%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# a simple deep learning NN from Scikit-Learn\n",
    "# (slow but quite accurate)\n",
    "\n",
    "acc = 0\n",
    "n_iter = 10\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(0,n_iter)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.25,random_state = state_list[i])\n",
    "    clf = neural_network.MLPClassifier(activation='tanh', solver='adam', tol=1e-05, hidden_layer_sizes=(8,8,8,8),\n",
    "                                       max_iter=300,learning_rate_init = 0.0001)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)\n",
    "    acc += accuracy_score(y_test,preds_bin) / n_iter\n",
    "\n",
    "t1 = time.time()\n",
    "eval_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running models ...\n",
      "MLPClassifier 0.9748\n",
      "RandomForestClassifier 0.9738\n",
      "LGBMClassifier 0.9748\n",
      "CatBoostClassifier 0.975\n",
      "KNeighborsClassifier 0.9736\n",
      "\n",
      "averaged predictions accuracy: 0.976\n",
      "hard voting accuracy: 0.9762\n"
     ]
    }
   ],
   "source": [
    "# now let's look at simple model emsembling\n",
    "# train different learners on the same data and combine the results\n",
    "\n",
    "# train test split again\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.25,random_state = 42)\n",
    "\n",
    "# create a dictionary of learning algorithms\n",
    "models = {'nn':neural_network.MLPClassifier(activation='tanh', solver='adam', tol=1e-05, \n",
    "                                            hidden_layer_sizes=(8,8,8,8),max_iter=300,\n",
    "                                            learning_rate_init = 0.0001),\n",
    "          'RandomForest':RandomForestClassifier(n_estimators=50,n_jobs=3),\n",
    "          'lgbm':lgb.LGBMClassifier(n_jobs = 3,n_estimators=50),\n",
    "          'catboost':CatBoostClassifier(logging_level='Silent',thread_count=3, n_estimators=50),\n",
    "          'knn':neighbors.KNeighborsClassifier(n_neighbors=25),\n",
    "          #'ExtraTrees':ExtraTreesClassifier(n_estimators=50,n_jobs=3),\n",
    "          #'XGBoost':xgb.XGBClassifier(n_estimators=50,n_jobs=3,tree_method='exact')\n",
    "         }\n",
    "\n",
    "# set up empty arrays to compute average and hard votes\n",
    "preds_array = np.zeros((len(models),len(y_test)))\n",
    "preds_avg = np.zeros(len(y_test))\n",
    "\n",
    "for i,name in enumerate(list(models.keys())):\n",
    "    if i==0: \n",
    "        print('running models ...') \n",
    "    #train and predict\n",
    "    clf = models[name]\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)\n",
    "    preds_proba = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # fill arrays\n",
    "    preds_array[i,:] = preds_bin\n",
    "    preds_avg += preds_proba / len(models)\n",
    "    \n",
    "    # accuracy of individual learners\n",
    "    acc = accuracy_score(y_test,preds_bin)\n",
    "    print(type(clf).__name__,np.round(acc,4))\n",
    "\n",
    "preds_vote = np.where(np.mean(preds_array,axis=0) > 0.5, 1, 0) \n",
    "\n",
    "# check accuracy of ensembles\n",
    "print('')\n",
    "print('averaged predictions accuracy:',accuracy_score(y_test,np.where(preds_avg > 0.5, 1, 0)))\n",
    "print('hard voting accuracy:',accuracy_score(y_test,preds_vote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running models ...\n",
      "RandomForestClassifier 0.974\n",
      "LGBMClassifier 0.9748\n",
      "CatBoostClassifier 0.975\n",
      "KNeighborsClassifier 0.9736\n",
      "MLPClassifier 0.975\n",
      "\n",
      "averaged predictions accuracy: 0.9764\n",
      "hard voting accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "# now let's look at simple model emsembling\n",
    "# train different learners on the same data and combine the results\n",
    "\n",
    "df2 = df.copy()\n",
    "\n",
    "#for col in cols:\n",
    "#    df2[col] = df[col].sample(frac=0.99)\n",
    "#df2.fillna(-99.9,inplace=True)\n",
    "\n",
    "#df2['Target'] = df2['Target'].sample(frac=0.7)\n",
    "#df2['Target'].fillna(0,inplace=True)\n",
    "#df2['Target'] = df2['Target'].sample(frac=0.7)\n",
    "#df2['Target'].fillna(1,inplace=True)\n",
    "#print(df2.head())\n",
    "\n",
    "# train test split again\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2[cols], df2['Target'],test_size=0.25,random_state = 42)\n",
    "\n",
    "# create a dictionary of learning algorithms\n",
    "models = {'RandomForest':RandomForestClassifier(n_estimators=50,n_jobs=3),\n",
    "          'lgbm':lgb.LGBMClassifier(n_jobs = 3,n_estimators=50),\n",
    "          'catboost':CatBoostClassifier(logging_level='Silent',thread_count=3, n_estimators=50),\n",
    "          'knn':neighbors.KNeighborsClassifier(n_neighbors=25),\n",
    "          'nn':neural_network.MLPClassifier(activation='tanh', solver='adam', tol=1e-05, \n",
    "                                            hidden_layer_sizes=(8,8,8,8),max_iter=300,\n",
    "                                            learning_rate_init = 0.0001),\n",
    "         }\n",
    "\n",
    "# set up empty arrays to compute average and hard votes\n",
    "preds_array = np.zeros((len(models),len(y_test)))\n",
    "preds_avg = np.zeros(len(y_test))\n",
    "\n",
    "for i,name in enumerate(list(models.keys())):\n",
    "    if i==0: \n",
    "        print('running models ...') \n",
    "    #train and predict\n",
    "    clf = models[name]\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)\n",
    "    preds_proba = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # fill arrays\n",
    "    preds_array[i,:] = preds_bin\n",
    "    preds_avg += preds_proba / len(models)\n",
    "    \n",
    "    # accuracy of individual learners\n",
    "    acc = accuracy_score(y_test,preds_bin)\n",
    "    print(type(clf).__name__,np.round(acc,4))\n",
    "\n",
    "preds_vote = np.where(np.mean(preds_array,axis=0) > 0.5, 1, 0) \n",
    "\n",
    "# check accuracy of ensembles\n",
    "print('')\n",
    "print('averaged predictions accuracy:',accuracy_score(y_test,np.where(preds_avg > 0.5, 1, 0)))\n",
    "print('hard voting accuracy:',accuracy_score(y_test,preds_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalised Stacking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual learners:\n",
      "accuracy score: 0.9384 (RandomForestClassifier)\n",
      "accuracy score: 0.9268 (LGBMClassifier)\n",
      "accuracy score: 0.9221 (CatBoostClassifier)\n",
      "accuracy score: 0.9154 (KNeighborsClassifier)\n",
      "accuracy score: 0.9475 (MLPClassifier)\n",
      "\n",
      "pseudo-labeling loop:\n",
      "starting model accuracy: 0.9268 (LGBMClassifier)\n",
      "1 pseudo labeling accuracy score: 0.9275 (LGBMClassifier)\n",
      "2 pseudo labeling accuracy score: 0.9482 (MLPClassifier)\n",
      "3 pseudo labeling accuracy score: 0.9489 (LGBMClassifier)\n",
      "4 pseudo labeling accuracy score: 0.9551 (MLPClassifier)\n",
      "5 pseudo labeling accuracy score: 0.9556 (LGBMClassifier)\n",
      "6 pseudo labeling accuracy score: 0.9561 (MLPClassifier)\n",
      "7 pseudo labeling accuracy score: 0.9567 (LGBMClassifier)\n",
      "8 pseudo labeling accuracy score: 0.9584 (MLPClassifier)\n",
      "9 pseudo labeling accuracy score: 0.9587 (LGBMClassifier)\n",
      "10 pseudo labeling accuracy score: 0.959 (MLPClassifier)\n",
      "11 pseudo labeling accuracy score: 0.9596 (LGBMClassifier)\n",
      "12 pseudo labeling accuracy score: 0.9596 (MLPClassifier)\n",
      "13 pseudo labeling accuracy score: 0.9602 (LGBMClassifier)\n",
      "14 pseudo labeling accuracy score: 0.9608 (MLPClassifier)\n",
      "15 pseudo labeling accuracy score: 0.9609 (LGBMClassifier)\n",
      "16 pseudo labeling accuracy score: 0.9618 (MLPClassifier)\n",
      "17 pseudo labeling accuracy score: 0.9623 (LGBMClassifier)\n",
      "18 pseudo labeling accuracy score: 0.9621 (MLPClassifier)\n",
      "19 pseudo labeling accuracy score: 0.9624 (LGBMClassifier)\n",
      "20 pseudo labeling accuracy score: 0.9624 (MLPClassifier)\n",
      "21 pseudo labeling accuracy score: 0.9629 (LGBMClassifier)\n",
      "22 pseudo labeling accuracy score: 0.963 (MLPClassifier)\n",
      "23 pseudo labeling accuracy score: 0.9634 (LGBMClassifier)\n",
      "24 pseudo labeling accuracy score: 0.9636 (MLPClassifier)\n",
      "25 pseudo labeling accuracy score: 0.9641 (LGBMClassifier)\n",
      "26 pseudo labeling accuracy score: 0.9639 (MLPClassifier)\n",
      "27 pseudo labeling accuracy score: 0.9646 (LGBMClassifier)\n",
      "28 pseudo labeling accuracy score: 0.9647 (MLPClassifier)\n",
      "29 pseudo labeling accuracy score: 0.9648 (LGBMClassifier)\n",
      "30 pseudo labeling accuracy score: 0.9655 (MLPClassifier)\n",
      "31 pseudo labeling accuracy score: 0.9655 (LGBMClassifier)\n",
      "32 pseudo labeling accuracy score: 0.9656 (MLPClassifier)\n",
      "33 pseudo labeling accuracy score: 0.9658 (LGBMClassifier)\n",
      "34 pseudo labeling accuracy score: 0.9656 (MLPClassifier)\n",
      "35 pseudo labeling accuracy score: 0.966 (LGBMClassifier)\n",
      "36 pseudo labeling accuracy score: 0.9659 (MLPClassifier)\n",
      "37 pseudo labeling accuracy score: 0.9662 (LGBMClassifier)\n",
      "38 pseudo labeling accuracy score: 0.9663 (MLPClassifier)\n",
      "39 pseudo labeling accuracy score: 0.9663 (LGBMClassifier)\n",
      "40 pseudo labeling accuracy score: 0.9668 (MLPClassifier)\n",
      "41 pseudo labeling accuracy score: 0.9669 (LGBMClassifier)\n",
      "42 pseudo labeling accuracy score: 0.9671 (MLPClassifier)\n",
      "43 pseudo labeling accuracy score: 0.9673 (LGBMClassifier)\n",
      "44 pseudo labeling accuracy score: 0.968 (MLPClassifier)\n",
      "45 pseudo labeling accuracy score: 0.9678 (LGBMClassifier)\n",
      "46 pseudo labeling accuracy score: 0.968 (MLPClassifier)\n",
      "47 pseudo labeling accuracy score: 0.9678 (LGBMClassifier)\n",
      "48 pseudo labeling accuracy score: 0.9682 (MLPClassifier)\n",
      "49 pseudo labeling accuracy score: 0.968 (LGBMClassifier)\n",
      "50 pseudo labeling accuracy score: 0.9678 (MLPClassifier)\n",
      "51 pseudo labeling accuracy score: 0.9678 (LGBMClassifier)\n",
      "52 pseudo labeling accuracy score: 0.9685 (MLPClassifier)\n",
      "53 pseudo labeling accuracy score: 0.9685 (LGBMClassifier)\n",
      "54 pseudo labeling accuracy score: 0.9687 (MLPClassifier)\n",
      "55 pseudo labeling accuracy score: 0.9686 (LGBMClassifier)\n",
      "56 pseudo labeling accuracy score: 0.9686 (MLPClassifier)\n",
      "57 pseudo labeling accuracy score: 0.9687 (LGBMClassifier)\n",
      "58 pseudo labeling accuracy score: 0.9686 (MLPClassifier)\n",
      "59 pseudo labeling accuracy score: 0.9686 (LGBMClassifier)\n",
      "60 pseudo labeling accuracy score: 0.9691 (MLPClassifier)\n",
      "61 pseudo labeling accuracy score: 0.9691 (LGBMClassifier)\n",
      "62 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "63 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "64 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "65 pseudo labeling accuracy score: 0.9693 (LGBMClassifier)\n",
      "66 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "67 pseudo labeling accuracy score: 0.9691 (LGBMClassifier)\n",
      "68 pseudo labeling accuracy score: 0.9691 (MLPClassifier)\n",
      "69 pseudo labeling accuracy score: 0.9689 (LGBMClassifier)\n",
      "70 pseudo labeling accuracy score: 0.9693 (MLPClassifier)\n",
      "71 pseudo labeling accuracy score: 0.9692 (LGBMClassifier)\n",
      "72 pseudo labeling accuracy score: 0.9692 (MLPClassifier)\n",
      "73 pseudo labeling accuracy score: 0.9692 (LGBMClassifier)\n",
      "74 pseudo labeling accuracy score: 0.9689 (MLPClassifier)\n",
      "75 pseudo labeling accuracy score: 0.9688 (LGBMClassifier)\n",
      "76 pseudo labeling accuracy score: 0.9691 (MLPClassifier)\n",
      "77 pseudo labeling accuracy score: 0.9689 (LGBMClassifier)\n",
      "78 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "79 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "80 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "81 pseudo labeling accuracy score: 0.9692 (LGBMClassifier)\n",
      "82 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "83 pseudo labeling accuracy score: 0.9693 (LGBMClassifier)\n",
      "84 pseudo labeling accuracy score: 0.969 (MLPClassifier)\n",
      "85 pseudo labeling accuracy score: 0.9693 (LGBMClassifier)\n",
      "86 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "87 pseudo labeling accuracy score: 0.9693 (LGBMClassifier)\n",
      "88 pseudo labeling accuracy score: 0.9691 (MLPClassifier)\n",
      "89 pseudo labeling accuracy score: 0.9689 (LGBMClassifier)\n",
      "90 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "91 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "92 pseudo labeling accuracy score: 0.9691 (MLPClassifier)\n",
      "93 pseudo labeling accuracy score: 0.9692 (LGBMClassifier)\n",
      "94 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "95 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "96 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "97 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "98 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "99 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "100 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "101 pseudo labeling accuracy score: 0.9692 (LGBMClassifier)\n",
      "102 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "103 pseudo labeling accuracy score: 0.9693 (LGBMClassifier)\n",
      "104 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "105 pseudo labeling accuracy score: 0.9696 (LGBMClassifier)\n",
      "106 pseudo labeling accuracy score: 0.9693 (MLPClassifier)\n",
      "107 pseudo labeling accuracy score: 0.9693 (LGBMClassifier)\n",
      "108 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "109 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "110 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "111 pseudo labeling accuracy score: 0.9696 (LGBMClassifier)\n",
      "112 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "113 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "114 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "115 pseudo labeling accuracy score: 0.9696 (LGBMClassifier)\n",
      "116 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "117 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "118 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "119 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "120 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "121 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "122 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "123 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "124 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "125 pseudo labeling accuracy score: 0.9696 (LGBMClassifier)\n",
      "126 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "127 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "128 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "129 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "130 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "131 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "132 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "133 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "134 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "135 pseudo labeling accuracy score: 0.9701 (LGBMClassifier)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "137 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "138 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "139 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "140 pseudo labeling accuracy score: 0.9701 (MLPClassifier)\n",
      "141 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "142 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "143 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "144 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "145 pseudo labeling accuracy score: 0.97 (LGBMClassifier)\n",
      "146 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "147 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "148 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "149 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "150 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "151 pseudo labeling accuracy score: 0.97 (LGBMClassifier)\n",
      "152 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "153 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "154 pseudo labeling accuracy score: 0.9702 (MLPClassifier)\n",
      "155 pseudo labeling accuracy score: 0.97 (LGBMClassifier)\n",
      "156 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "157 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "158 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "159 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "160 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "161 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "162 pseudo labeling accuracy score: 0.9691 (MLPClassifier)\n",
      "163 pseudo labeling accuracy score: 0.9691 (LGBMClassifier)\n",
      "164 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "165 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "166 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "167 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "168 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "169 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "170 pseudo labeling accuracy score: 0.9693 (MLPClassifier)\n",
      "171 pseudo labeling accuracy score: 0.9692 (LGBMClassifier)\n",
      "172 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "173 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "174 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "175 pseudo labeling accuracy score: 0.9692 (LGBMClassifier)\n",
      "176 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "177 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "178 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "179 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "180 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "181 pseudo labeling accuracy score: 0.9693 (LGBMClassifier)\n",
      "182 pseudo labeling accuracy score: 0.9693 (MLPClassifier)\n",
      "183 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "184 pseudo labeling accuracy score: 0.9693 (MLPClassifier)\n",
      "185 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "186 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "187 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "188 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "189 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "190 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "191 pseudo labeling accuracy score: 0.9696 (LGBMClassifier)\n",
      "192 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "193 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "194 pseudo labeling accuracy score: 0.9694 (MLPClassifier)\n",
      "195 pseudo labeling accuracy score: 0.9695 (LGBMClassifier)\n",
      "196 pseudo labeling accuracy score: 0.9695 (MLPClassifier)\n",
      "197 pseudo labeling accuracy score: 0.9694 (LGBMClassifier)\n",
      "198 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "199 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "200 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "201 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "202 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "203 pseudo labeling accuracy score: 0.9696 (LGBMClassifier)\n",
      "204 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "205 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "206 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "207 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "208 pseudo labeling accuracy score: 0.9702 (MLPClassifier)\n",
      "209 pseudo labeling accuracy score: 0.9701 (LGBMClassifier)\n",
      "210 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "211 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "212 pseudo labeling accuracy score: 0.9701 (MLPClassifier)\n",
      "213 pseudo labeling accuracy score: 0.97 (LGBMClassifier)\n",
      "214 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "215 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "216 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "217 pseudo labeling accuracy score: 0.9696 (LGBMClassifier)\n",
      "218 pseudo labeling accuracy score: 0.9702 (MLPClassifier)\n",
      "219 pseudo labeling accuracy score: 0.97 (LGBMClassifier)\n",
      "220 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n",
      "221 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "222 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "223 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "224 pseudo labeling accuracy score: 0.9701 (MLPClassifier)\n",
      "225 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "226 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "227 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "228 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "229 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "230 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "231 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "232 pseudo labeling accuracy score: 0.9704 (MLPClassifier)\n",
      "233 pseudo labeling accuracy score: 0.9702 (LGBMClassifier)\n",
      "234 pseudo labeling accuracy score: 0.9702 (MLPClassifier)\n",
      "235 pseudo labeling accuracy score: 0.9702 (LGBMClassifier)\n",
      "236 pseudo labeling accuracy score: 0.9703 (MLPClassifier)\n",
      "237 pseudo labeling accuracy score: 0.9701 (LGBMClassifier)\n",
      "238 pseudo labeling accuracy score: 0.9703 (MLPClassifier)\n",
      "239 pseudo labeling accuracy score: 0.9702 (LGBMClassifier)\n",
      "240 pseudo labeling accuracy score: 0.9703 (MLPClassifier)\n",
      "241 pseudo labeling accuracy score: 0.9702 (LGBMClassifier)\n",
      "242 pseudo labeling accuracy score: 0.9704 (MLPClassifier)\n",
      "243 pseudo labeling accuracy score: 0.9703 (LGBMClassifier)\n",
      "244 pseudo labeling accuracy score: 0.97 (MLPClassifier)\n",
      "245 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "246 pseudo labeling accuracy score: 0.97 (MLPClassifier)\n",
      "247 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "248 pseudo labeling accuracy score: 0.9697 (MLPClassifier)\n",
      "249 pseudo labeling accuracy score: 0.9696 (LGBMClassifier)\n",
      "250 pseudo labeling accuracy score: 0.97 (MLPClassifier)\n",
      "251 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "252 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "253 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "254 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "255 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "256 pseudo labeling accuracy score: 0.9696 (MLPClassifier)\n",
      "257 pseudo labeling accuracy score: 0.9696 (LGBMClassifier)\n",
      "258 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "259 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "260 pseudo labeling accuracy score: 0.97 (MLPClassifier)\n",
      "261 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "262 pseudo labeling accuracy score: 0.9701 (MLPClassifier)\n",
      "263 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "264 pseudo labeling accuracy score: 0.9701 (MLPClassifier)\n",
      "265 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "266 pseudo labeling accuracy score: 0.9702 (MLPClassifier)\n",
      "267 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "268 pseudo labeling accuracy score: 0.97 (MLPClassifier)\n",
      "269 pseudo labeling accuracy score: 0.9701 (LGBMClassifier)\n",
      "270 pseudo labeling accuracy score: 0.9701 (MLPClassifier)\n",
      "271 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "272 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "273 pseudo labeling accuracy score: 0.9698 (LGBMClassifier)\n",
      "274 pseudo labeling accuracy score: 0.9698 (MLPClassifier)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 pseudo labeling accuracy score: 0.9697 (LGBMClassifier)\n",
      "276 pseudo labeling accuracy score: 0.9703 (MLPClassifier)\n",
      "277 pseudo labeling accuracy score: 0.97 (LGBMClassifier)\n",
      "278 pseudo labeling accuracy score: 0.9699 (MLPClassifier)\n",
      "279 pseudo labeling accuracy score: 0.9699 (LGBMClassifier)\n",
      "280 pseudo labeling accuracy score: 0.9704 (MLPClassifier)\n",
      "281 pseudo labeling accuracy score: 0.9704 (LGBMClassifier)\n",
      "282 pseudo labeling accuracy score: 0.9707 (MLPClassifier)\n",
      "283 pseudo labeling accuracy score: 0.9706 (LGBMClassifier)\n",
      "284 pseudo labeling accuracy score: 0.9706 (MLPClassifier)\n",
      "285 pseudo labeling accuracy score: 0.9705 (LGBMClassifier)\n",
      "286 pseudo labeling accuracy score: 0.9705 (MLPClassifier)\n",
      "287 pseudo labeling accuracy score: 0.9704 (LGBMClassifier)\n",
      "288 pseudo labeling accuracy score: 0.9705 (MLPClassifier)\n",
      "289 pseudo labeling accuracy score: 0.9704 (LGBMClassifier)\n",
      "290 pseudo labeling accuracy score: 0.9708 (MLPClassifier)\n",
      "291 pseudo labeling accuracy score: 0.9705 (LGBMClassifier)\n",
      "292 pseudo labeling accuracy score: 0.9703 (MLPClassifier)\n",
      "293 pseudo labeling accuracy score: 0.97 (LGBMClassifier)\n",
      "294 pseudo labeling accuracy score: 0.9707 (MLPClassifier)\n",
      "295 pseudo labeling accuracy score: 0.9704 (LGBMClassifier)\n",
      "296 pseudo labeling accuracy score: 0.9704 (MLPClassifier)\n",
      "297 pseudo labeling accuracy score: 0.9703 (LGBMClassifier)\n",
      "298 pseudo labeling accuracy score: 0.9706 (MLPClassifier)\n",
      "299 pseudo labeling accuracy score: 0.9706 (LGBMClassifier)\n",
      "300 pseudo labeling accuracy score: 0.9704 (MLPClassifier)\n",
      "done: (5795.2) s\n"
     ]
    }
   ],
   "source": [
    "# pseudo labeling: an advanced semi-supervised method\n",
    "# let's apply this method using catboost \n",
    "\n",
    "print('individual learners:')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.975)\n",
    "# evaluate accuracy of individual learners\n",
    "acc_best = 0\n",
    "for model in models.keys():\n",
    "    clf = models[model]\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test,preds_bin)\n",
    "    print('accuracy score:',np.round(acc,4),'('+str(type(clf).__name__)+')')\n",
    "\n",
    "# choose a learner for initial model:\n",
    "#clf = models['nn']\n",
    "clf = models['lgbm']\n",
    "clf.fit(X_train, y_train)\n",
    "preds_bin = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test,preds_bin)\n",
    "print('')\n",
    "print('pseudo-labeling loop:')\n",
    "print('starting model accuracy:',np.round(acc,4),'('+str(type(clf).__name__)+')')\n",
    "    \n",
    "# create new training set by merging train and test data\n",
    "X_train_p = pd.concat([X_train,X_test])\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(0,300):\n",
    "    # generate new targets by merging train targets and test predictions (i.e., pseudo labels)\n",
    "    y_train_p = y_train.tolist() + preds_bin.tolist()\n",
    "    \n",
    "    # alternate between tree and deep learning models\n",
    "    if i % 2 != 0: model = 'nn'\n",
    "    else: model = 'lgbm'\n",
    "    \n",
    "    clf = models[model]\n",
    "    clf.fit(X_train_p, y_train_p)\n",
    "    preds_bin = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test,preds_bin)\n",
    "    t1 = time.time()\n",
    "    print(i+1,'pseudo labeling accuracy score:',np.round(acc,4),'('+str(type(clf).__name__)+')')\n",
    "\n",
    "t1 = time.time()\n",
    "print('done: ('+str(np.round(t1-t0,1))+') s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual learners:\n",
      "accuracy score: 0.9718 (RandomForestClassifier)\n",
      "accuracy score: 0.9739 (LGBMClassifier)\n",
      "accuracy score: 0.973 (CatBoostClassifier)\n",
      "accuracy score: 0.9717 (KNeighborsClassifier)\n",
      "accuracy score: 0.9727 (MLPClassifier)\n",
      "\n",
      "pseudo-labeling loop:\n",
      "starting model accuracy: 0.9739 (LGBMClassifier)\n",
      "1 pseudo labeling accuracy score: 0.9735 (MLPClassifier)\n",
      "2 pseudo labeling accuracy score: 0.9735 (LGBMClassifier)\n",
      "3 pseudo labeling accuracy score: 0.9729 (MLPClassifier)\n",
      "done: (96.6) s\n"
     ]
    }
   ],
   "source": [
    "# pseudo labeling: same thing, smaller test data size\n",
    "\n",
    "print('individual learners:')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cols], df['Target'],test_size=0.33)\n",
    "# evaluate accuracy of individual learners\n",
    "acc_best = 0\n",
    "for model in models.keys():\n",
    "    clf = models[model]\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_bin = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test,preds_bin)  \n",
    "    print('accuracy score:',np.round(acc,4),'('+str(type(clf).__name__)+')')\n",
    "\n",
    "# choose a learner for initial model:\n",
    "#clf = models['nn']\n",
    "clf = models['lgbm']\n",
    "clf.fit(X_train, y_train)\n",
    "preds_bin = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test,preds_bin)\n",
    "print('')\n",
    "print('pseudo-labeling loop:')\n",
    "print('starting model accuracy:',np.round(acc,4),'('+str(type(clf).__name__)+')')\n",
    "    \n",
    "# create new training set by merging train and test data\n",
    "X_train_p = pd.concat([X_train,X_test])\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(0,3):\n",
    "    # generate new targets by merging train targets and test predictions (i.e., pseudo labels)\n",
    "    y_train_p = y_train.tolist() + preds_bin.tolist()\n",
    "    \n",
    "    # alternate between tree and deep learning models\n",
    "    if i % 2 == 0: model = 'nn'\n",
    "    else: model = 'lgbm'\n",
    "    \n",
    "    clf = models[model]\n",
    "    clf.fit(X_train_p, y_train_p)\n",
    "    preds_bin = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test,preds_bin)\n",
    "    t1 = time.time()\n",
    "    print(i+1,'pseudo labeling accuracy score:',np.round(acc,4),'('+str(type(clf).__name__)+')')\n",
    "\n",
    "t1 = time.time()\n",
    "print('done: ('+str(np.round(t1-t0,1))+') s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
